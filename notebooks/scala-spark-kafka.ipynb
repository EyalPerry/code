{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.0`\n",
    "import $ivy.`org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0`\n",
    "\n",
    "import $ivy.`org.apache.kafka:kafka-clients:3.5.1`\n",
    "import $ivy.`org.testcontainers:testcontainers:1.19.3`\n",
    "import $ivy.`org.testcontainers:kafka:1.19.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 deprecation; re-run enabling -deprecation for details, or try -help\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/30 18:12:59 WARN Utils: Your hostname, DESKTOP-CN01VS3 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/30 18:12:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/30 18:12:59 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/03/30 18:12:59 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 18:12:59 INFO SparkContext: Java version 17.0.14\n",
      "25/03/30 18:12:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 18:12:59 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 18:12:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/30 18:12:59 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 18:12:59 INFO SparkContext: Submitted application: ScalaSpark\n",
      "25/03/30 18:12:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/30 18:12:59 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/30 18:12:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/30 18:12:59 INFO SecurityManager: Changing view acls to: eyal\n",
      "25/03/30 18:12:59 INFO SecurityManager: Changing modify acls to: eyal\n",
      "25/03/30 18:12:59 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/30 18:12:59 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/30 18:12:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: eyal; groups with view permissions: EMPTY; users with modify permissions: eyal; groups with modify permissions: EMPTY\n",
      "25/03/30 18:13:00 INFO Utils: Successfully started service 'sparkDriver' on port 34707.\n",
      "25/03/30 18:13:00 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/30 18:13:00 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/30 18:13:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/30 18:13:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/30 18:13:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/30 18:13:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60600458-1c68-4e92-ade0-974074d991bc\n",
      "25/03/30 18:13:00 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "25/03/30 18:13:00 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/30 18:13:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/03/30 18:13:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/30 18:13:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/30 18:13:00 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "25/03/30 18:13:00 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/03/30 18:13:00 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 18:13:00 INFO Executor: Java version 17.0.14\n",
      "25/03/30 18:13:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/30 18:13:00 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73e51894 for default.\n",
      "25/03/30 18:13:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34811.\n",
      "25/03/30 18:13:00 INFO NettyBlockTransferService: Server created on 10.255.255.254:34811\n",
      "25/03/30 18:13:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/30 18:13:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 34811, None)\n",
      "25/03/30 18:13:00 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:34811 with 2.2 GiB RAM, BlockManagerId(driver, 10.255.255.254, 34811, None)\n",
      "25/03/30 18:13:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 34811, None)\n",
      "25/03/30 18:13:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 34811, None)\n",
      "25/03/30 18:13:00 INFO PullPolicy: Image pull policy will be performed by: DefaultPullPolicy()\n",
      "25/03/30 18:13:00 INFO ImageNameSubstitutor: Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')\n",
      "25/03/30 18:13:01 INFO DockerClientProviderStrategy: Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first\n",
      "25/03/30 18:13:01 INFO DockerClientProviderStrategy: Found Docker environment with local Unix socket (unix:///var/run/docker.sock)\n",
      "25/03/30 18:13:01 INFO DockerClientFactory: Docker host IP address is localhost\n",
      "25/03/30 18:13:01 INFO DockerClientFactory: Connected to docker: \n",
      "  Server Version: 28.0.1\n",
      "  API Version: 1.48\n",
      "  Operating System: Docker Desktop\n",
      "  Total Memory: 15954 MB\n",
      "25/03/30 18:13:01 INFO 1: Creating container for image: testcontainers/ryuk:0.5.1\n",
      "25/03/30 18:13:01 INFO 1: Container testcontainers/ryuk:0.5.1 is starting: 5d5d6fc9fd84b71b8d360e87b200bd36e1ca8f66d02838bb70fc9116692aa797\n",
      "25/03/30 18:13:01 INFO 1: Container testcontainers/ryuk:0.5.1 started in PT0.46779227S\n",
      "25/03/30 18:13:01 INFO RyukResourceReaper: Ryuk started - will monitor and terminate Testcontainers containers on JVM exit\n",
      "25/03/30 18:13:01 INFO DockerClientFactory: Checking the system...\n",
      "25/03/30 18:13:01 INFO DockerClientFactory: ✔︎ Docker server version should be at least 1.6.0\n",
      "25/03/30 18:13:01 INFO 3: Creating container for image: confluentinc/cp-kafka:5.4.3\n",
      "25/03/30 18:13:01 INFO 3: Container confluentinc/cp-kafka:5.4.3 is starting: cad2b40225b0b62630e5bb0ccc50654b6116952e82413839bf4c961439563d25\n",
      "25/03/30 18:13:05 INFO 3: Container confluentinc/cp-kafka:5.4.3 started in PT3.944761587S\n",
      "25/03/30 18:13:05 INFO ProducerConfig: ProducerConfig values: \n",
      "\tacks = -1\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbatch.size = 16384\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:59675]\n",
      "\tbuffer.memory = 33554432\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = producer-1\n",
      "\tcompression.type = none\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdelivery.timeout.ms = 120000\n",
      "\tenable.idempotence = true\n",
      "\tinterceptor.classes = []\n",
      "\tkey.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\tlinger.ms = 0\n",
      "\tmax.block.ms = 60000\n",
      "\tmax.in.flight.requests.per.connection = 5\n",
      "\tmax.request.size = 1048576\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetadata.max.idle.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartitioner.adaptive.partitioning.enable = true\n",
      "\tpartitioner.availability.timeout.ms = 0\n",
      "\tpartitioner.class = null\n",
      "\tpartitioner.ignore.keys = false\n",
      "\treceive.buffer.bytes = 32768\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.timeout.ms = 60000\n",
      "\ttransactional.id = null\n",
      "\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\n",
      "25/03/30 18:13:05 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.\n",
      "25/03/30 18:13:05 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 18:13:05 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 18:13:05 INFO AppInfoParser: Kafka startTimeMs: 1743347585884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.kafka.clients.producer._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.containers.KafkaContainer\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.utility.DockerImageName\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@164969f0\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mkafkaContainer\u001b[39m: \u001b[32mKafkaContainer\u001b[39m = GenericContainer(extraHosts=[], image=RemoteDockerImage(imageName=confluentinc/cp-kafka:5.4.3, imagePullPolicy=DefaultPullPolicy(), imageNameSubstitutor=org.testcontainers.utility.ImageNameSubstitutor$LogWrappedImageNameSubstitutor@624908a7), volumesFroms=[], linkedContainers={}, startupCheckStrategy=org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy@616a1cfe, startupAttempts=1, workingDirectory=null, shmSize=null, copyToFileContainerPathMap={}, copyToTransferableContainerPathMap={}, dependencies=[], dockerClient=LazyDockerClient, containerId=cad2b40225b0b62630e5bb0ccc50654b6116952e82413839bf4c961439563d25, containerInfo=InspectContainerResponse(args=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], config=ContainerConfig(attachStderr=false, attachStdin=false, attachStdout=false, cmd=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], domainName=, entrypoint=[sh], env=[KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807, KAFKA_BROKER_ID=1, KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1, KAFKA_ZOOKEEPER_CONNECT=localhost:2181, KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1, KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1, KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092, KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1, KAFKA_INTER_BROKER_LISTENER_NAME=BROKER, KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT, KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0, PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin, ALLOW_UNSIGNED=false, PYTHON_VERSION=2.7.9-1, SCALA_VERSION=2.12, KAFKA_VERSION=, CONFLUENT_PLATFORM_LABEL=, CONFLUENT_VERSION=5.4.3, CONFLUENT_DEB_VERSION=1, ZULU_OPENJDK_VERSION=8=8.40.0.25, LANG=C.UTF-8, CUB_CLASSPATH=/etc/confluent/docker/docker-utils.jar, KAFKA_ADVERTISED_LISTENERS=, COMPONENT=kafka], exposedPorts=[2181/tcp, 9092/tcp, 9093/tcp], hostName=cad2b40225b0, image=confluentinc/cp-kafka:5.4.3, labels={desktop.docker.io/wsl-distro=Ubuntu-24.04, io.confluent.docker=true, io.confluent.docker.build.number=2, io.confluent.docker.git.id=af0a986, io.confluent.docker.git.repo=confluentinc/kafka-images, maintainer=partner-support@confluent.io, org.testcontainers=true, org.testcontainers.lang=java, org.testcontainers.sessionId=de4dacfe-f9b7-45a5-911e-ae89497cfc9b, org.testcontainers.version=1.19.3}, macAddress=null, networkDisabled=null, onBuild=null, stdinOpen=false, portSpecs=null, stdInOnce=false, tty=false, user=0, volumes={/etc/kafka/secrets={}, /var/lib/kafka/data={}}, workingDir=, healthCheck=null), created=2025-03-30T15:13:01.841092115Z, driver=overlay2, execDriver=null, hostConfig=HostConfig(binds=[], blkioWeight=0, blkioWeightDevice=null, blkioDeviceReadBps=null, blkioDeviceWriteBps=null, blkioDeviceReadIOps=null, blkioDeviceWriteIOps=null, memorySwappiness=null, nanoCPUs=0, capAdd=null, capDrop=null, containerIDFile=, cpuPeriod=0, cpuRealtimePeriod=0, cpuRealtimeRuntime=0, cpuShares=0, cpuQuota=0, cpusetCpus=, cpusetMems=, devices=null, deviceCgroupRules=null, deviceRequests=null, diskQuota=null, dns=null, dnsOptions=nu...\n",
       "\u001b[36mbootstrapServers\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"PLAINTEXT://localhost:59675\"\u001b[39m\n",
       "\u001b[36mkafkaProps\u001b[39m: \u001b[32mProperties\u001b[39m = {value.serializer=org.apache.kafka.common.serialization.StringSerializer, bootstrap.servers=PLAINTEXT://localhost:59675, key.serializer=org.apache.kafka.common.serialization.StringSerializer}\n",
       "\u001b[36mres2_11\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_12\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_13\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mkafkaProducer\u001b[39m: \u001b[32mKafkaProducer\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.kafka.clients.producer.KafkaProducer@3294dec9\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msendToKafka\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mreadFromKafkaSpark\u001b[39m\n",
       "\u001b[36mkafkaTopic\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"my-topic-scala\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.kafka.clients.producer._\n",
    "\n",
    "import org.testcontainers.containers.KafkaContainer\n",
    "import org.testcontainers.utility.DockerImageName\n",
    "import java.util.Properties\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"ScalaSpark\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val kafkaContainer = new KafkaContainer()\n",
    "\n",
    "kafkaContainer.start()\n",
    "\n",
    "val bootstrapServers = kafkaContainer.getBootstrapServers\n",
    "val kafkaProps = new Properties()\n",
    "kafkaProps.put(\"bootstrap.servers\", bootstrapServers)\n",
    "kafkaProps.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "kafkaProps.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val kafkaProducer = new KafkaProducer[String, String](kafkaProps)\n",
    "\n",
    "def sendToKafka(topic: String, values: Seq[String]): Unit = {\n",
    " \n",
    "  values.foreach { value =>\n",
    "    val record = new ProducerRecord[String, String](topic, value)\n",
    "    kafkaProducer.send(record)\n",
    "  }\n",
    "  kafkaProducer.flush()\n",
    "}\n",
    "\n",
    "def readFromKafkaSpark(topic: String): DataFrame = {\n",
    "  spark.read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", bootstrapServers)\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load()\n",
    "  .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "}\n",
    "\n",
    "val kafkaTopic = \"my-topic-scala\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 18:13:08 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {my-topic-scala=LEADER_NOT_AVAILABLE}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"hi\"\u001b[39m, \u001b[32m\"there\"\u001b[39m, \u001b[32m\"from\"\u001b[39m, \u001b[32m\"scala\"\u001b[39m)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"hi\", \"there\", \"from\", \"scala\")\n",
    "\n",
    "sendToKafka(kafkaTopic, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 18:13:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/30 18:13:09 INFO SharedState: Warehouse path is 'file:/home/eyal/code/notebooks/spark-warehouse'.\n",
      "25/03/30 18:13:09 INFO CodeGenerator: Code generated in 175.640916 ms\n",
      "25/03/30 18:13:09 INFO CodeGenerator: Code generated in 23.892742 ms\n",
      "25/03/30 18:13:10 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:59675]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/03/30 18:13:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/03/30 18:13:10 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 18:13:10 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 18:13:10 INFO AppInfoParser: Kafka startTimeMs: 1743347590798\n",
      "25/03/30 18:13:10 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered\n",
      "25/03/30 18:13:10 INFO Metrics: Metrics scheduler closed\n",
      "25/03/30 18:13:10 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/03/30 18:13:10 INFO Metrics: Metrics reporters closed\n",
      "25/03/30 18:13:10 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(my-topic-scala-0,-2,-1,None)\n",
      "25/03/30 18:13:11 INFO CodeGenerator: Code generated in 20.472726 ms\n",
      "25/03/30 18:13:11 INFO CodeGenerator: Code generated in 10.009312 ms\n",
      "25/03/30 18:13:11 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Got job 0 (show at cmd4.sc:1) with 1 output partitions\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd4.sc:1)\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1), which has no missing parents\n",
      "25/03/30 18:13:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 31.5 KiB, free 2.2 GiB)\n",
      "25/03/30 18:13:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 2.2 GiB)\n",
      "25/03/30 18:13:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:34811 (size: 11.1 KiB, free: 2.2 GiB)\n",
      "25/03/30 18:13:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "25/03/30 18:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/30 18:13:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/03/30 18:13:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 7883 bytes) \n",
      "25/03/30 18:13:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/03/30 18:13:11 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:59675]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/03/30 18:13:11 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 18:13:11 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 18:13:11 INFO AppInfoParser: Kafka startTimeMs: 1743347591700\n",
      "25/03/30 18:13:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Assigned to partition(s): my-topic-scala-0\n",
      "25/03/30 18:13:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 18:13:11 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Cluster ID: X0nhruEqRnmduBz11Wn3Wg\n",
      "25/03/30 18:13:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:59675 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 18:13:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 18:13:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:59675 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 18:13:11 INFO CodeGenerator: Code generated in 12.0822 ms\n",
      "25/03/30 18:13:12 INFO CodeGenerator: Code generated in 16.906691 ms\n",
      "25/03/30 18:13:12 INFO CodeGenerator: Code generated in 6.974236 ms\n",
      "25/03/30 18:13:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Seeking to offset 0 for partition my-topic-scala-0\n",
      "25/03/30 18:13:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 18:13:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:59675 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 18:13:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 18:13:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor-1, groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:59675 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 18:13:12 INFO CodeGenerator: Code generated in 33.584184 ms\n",
      "25/03/30 18:13:12 INFO KafkaDataConsumer: From Kafka topicPartition=my-topic-scala-0 groupId=spark-kafka-relation-3c634fc0-9950-4309-b39b-c5382581d222-executor read 4 records through 1 polls (polled  out 4 records), taking 593484407 nanos, during time span of 1184667923 nanos.\n",
      "25/03/30 18:13:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1761 bytes result sent to driver\n",
      "25/03/30 18:13:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1413 ms on 10.255.255.254 (executor driver) (1/1)\n",
      "25/03/30 18:13:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/03/30 18:13:12 INFO DAGScheduler: ResultStage 0 (show at cmd4.sc:1) finished in 1.579 s\n",
      "25/03/30 18:13:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/30 18:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/03/30 18:13:12 INFO DAGScheduler: Job 0 finished: show at cmd4.sc:1, took 1.626354 s\n",
      "25/03/30 18:13:12 INFO CodeGenerator: Code generated in 6.051043 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|message|\n",
      "+-------+\n",
      "|     hi|\n",
      "|  there|\n",
      "|   from|\n",
      "|  scala|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readFromKafkaSpark(kafkaTopic).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
