{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.0`\n",
    "import $ivy.`org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0`\n",
    "\n",
    "import $ivy.`org.apache.kafka:kafka-clients:3.5.1`\n",
    "import $ivy.`org.testcontainers:testcontainers:1.19.3`\n",
    "import $ivy.`org.testcontainers:kafka:1.19.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 deprecation; re-run enabling -deprecation for details, or try -help\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/30 17:34:10 WARN Utils: Your hostname, DESKTOP-CN01VS3 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/30 17:34:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/30 17:34:10 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/03/30 17:34:10 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 17:34:10 INFO SparkContext: Java version 17.0.14\n",
      "25/03/30 17:34:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 17:34:10 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 17:34:10 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/30 17:34:10 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 17:34:10 INFO SparkContext: Submitted application: ScalaSpark\n",
      "25/03/30 17:34:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/30 17:34:10 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/30 17:34:10 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/30 17:34:10 INFO SecurityManager: Changing view acls to: eyal\n",
      "25/03/30 17:34:10 INFO SecurityManager: Changing modify acls to: eyal\n",
      "25/03/30 17:34:10 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/30 17:34:10 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/30 17:34:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: eyal; groups with view permissions: EMPTY; users with modify permissions: eyal; groups with modify permissions: EMPTY\n",
      "25/03/30 17:34:10 INFO Utils: Successfully started service 'sparkDriver' on port 32959.\n",
      "25/03/30 17:34:10 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/30 17:34:10 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/30 17:34:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/30 17:34:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/30 17:34:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/30 17:34:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-946b29e4-7fa7-4c60-bbd6-a45d038fff80\n",
      "25/03/30 17:34:10 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "25/03/30 17:34:10 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/30 17:34:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/03/30 17:34:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/30 17:34:11 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/03/30 17:34:11 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/03/30 17:34:11 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 17:34:11 INFO Executor: Java version 17.0.14\n",
      "25/03/30 17:34:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/30 17:34:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1b5e58f1 for default.\n",
      "25/03/30 17:34:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38805.\n",
      "25/03/30 17:34:11 INFO NettyBlockTransferService: Server created on 10.255.255.254:38805\n",
      "25/03/30 17:34:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/30 17:34:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 38805, None)\n",
      "25/03/30 17:34:11 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:38805 with 2.2 GiB RAM, BlockManagerId(driver, 10.255.255.254, 38805, None)\n",
      "25/03/30 17:34:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 38805, None)\n",
      "25/03/30 17:34:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 38805, None)\n",
      "25/03/30 17:34:11 INFO PullPolicy: Image pull policy will be performed by: DefaultPullPolicy()\n",
      "25/03/30 17:34:11 INFO ImageNameSubstitutor: Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')\n",
      "25/03/30 17:34:11 INFO DockerClientProviderStrategy: Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first\n",
      "25/03/30 17:34:11 INFO DockerClientProviderStrategy: Found Docker environment with local Unix socket (unix:///var/run/docker.sock)\n",
      "25/03/30 17:34:11 INFO DockerClientFactory: Docker host IP address is localhost\n",
      "25/03/30 17:34:11 INFO DockerClientFactory: Connected to docker: \n",
      "  Server Version: 28.0.1\n",
      "  API Version: 1.48\n",
      "  Operating System: Docker Desktop\n",
      "  Total Memory: 15954 MB\n",
      "25/03/30 17:34:11 INFO 1: Creating container for image: testcontainers/ryuk:0.5.1\n",
      "25/03/30 17:34:12 INFO 1: Container testcontainers/ryuk:0.5.1 is starting: a3f932393ee239918dfc5e5a8b241d8dfe658471403aee0d4c18162e3d35ebc0\n",
      "25/03/30 17:34:12 INFO 1: Container testcontainers/ryuk:0.5.1 started in PT0.496387406S\n",
      "25/03/30 17:34:12 INFO RyukResourceReaper: Ryuk started - will monitor and terminate Testcontainers containers on JVM exit\n",
      "25/03/30 17:34:12 INFO DockerClientFactory: Checking the system...\n",
      "25/03/30 17:34:12 INFO DockerClientFactory: ✔︎ Docker server version should be at least 1.6.0\n",
      "25/03/30 17:34:12 INFO 3: Pulling docker image: confluentinc/cp-kafka:5.4.3. Please be patient; this may take some time but only needs to be done once.\n",
      "25/03/30 17:34:14 INFO 3: Starting to pull image\n",
      "25/03/30 17:34:14 INFO 3: Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes)\n",
      "25/03/30 17:34:15 INFO 3: Pulling image layers:  7 pending,  1 downloaded,  0 extracted, (524 KB/? MB)\n",
      "25/03/30 17:34:16 INFO 3: Pulling image layers:  6 pending,  2 downloaded,  0 extracted, (55 MB/? MB)\n",
      "25/03/30 17:34:17 INFO 3: Pulling image layers:  5 pending,  3 downloaded,  0 extracted, (139 MB/? MB)\n",
      "25/03/30 17:34:19 INFO 3: Pulling image layers:  4 pending,  4 downloaded,  0 extracted, (233 MB/? MB)\n",
      "25/03/30 17:34:20 INFO 3: Pulling image layers:  3 pending,  5 downloaded,  0 extracted, (286 MB/? MB)\n",
      "25/03/30 17:34:20 INFO 3: Pulling image layers:  2 pending,  6 downloaded,  0 extracted, (299 MB/? MB)\n",
      "25/03/30 17:34:20 INFO 3: Pulling image layers:  1 pending,  7 downloaded,  0 extracted, (304 MB/? MB)\n",
      "25/03/30 17:34:21 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  0 extracted, (304 MB/314 MB)\n",
      "25/03/30 17:34:21 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  1 extracted, (306 MB/314 MB)\n",
      "25/03/30 17:34:21 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  2 extracted, (306 MB/314 MB)\n",
      "25/03/30 17:34:28 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  3 extracted, (307 MB/314 MB)\n",
      "25/03/30 17:34:28 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  4 extracted, (307 MB/314 MB)\n",
      "25/03/30 17:34:28 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  5 extracted, (308 MB/314 MB)\n",
      "25/03/30 17:34:28 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  6 extracted, (314 MB/314 MB)\n",
      "25/03/30 17:34:28 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  7 extracted, (314 MB/314 MB)\n",
      "25/03/30 17:34:29 INFO 3: Pulling image layers:  0 pending,  8 downloaded,  8 extracted, (314 MB/314 MB)\n",
      "25/03/30 17:34:29 INFO 3: Image confluentinc/cp-kafka:5.4.3 pull took PT16.616433487S\n",
      "25/03/30 17:34:29 INFO 3: Pull complete. 8 layers, pulled in 14s (downloaded 314 MB at 22 MB/s)\n",
      "25/03/30 17:34:29 INFO 3: Creating container for image: confluentinc/cp-kafka:5.4.3\n",
      "25/03/30 17:34:29 INFO 3: Container confluentinc/cp-kafka:5.4.3 is starting: 1b2790461c4707b170e4ecd66efb7a75461da6e9701ffaec950969fad187f09f\n",
      "25/03/30 17:34:33 INFO 3: Container confluentinc/cp-kafka:5.4.3 started in PT4.286873582S\n",
      "25/03/30 17:34:33 INFO ProducerConfig: ProducerConfig values: \n",
      "\tacks = -1\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbatch.size = 16384\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:58094]\n",
      "\tbuffer.memory = 33554432\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = producer-1\n",
      "\tcompression.type = none\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdelivery.timeout.ms = 120000\n",
      "\tenable.idempotence = true\n",
      "\tinterceptor.classes = []\n",
      "\tkey.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\tlinger.ms = 0\n",
      "\tmax.block.ms = 60000\n",
      "\tmax.in.flight.requests.per.connection = 5\n",
      "\tmax.request.size = 1048576\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetadata.max.idle.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartitioner.adaptive.partitioning.enable = true\n",
      "\tpartitioner.availability.timeout.ms = 0\n",
      "\tpartitioner.class = null\n",
      "\tpartitioner.ignore.keys = false\n",
      "\treceive.buffer.bytes = 32768\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.timeout.ms = 60000\n",
      "\ttransactional.id = null\n",
      "\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\n",
      "25/03/30 17:34:33 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.\n",
      "25/03/30 17:34:33 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 17:34:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 17:34:33 INFO AppInfoParser: Kafka startTimeMs: 1743345273441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.kafka.clients.producer._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.containers.KafkaContainer\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.utility.DockerImageName\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4f1d1fe3\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mkafkaContainer\u001b[39m: \u001b[32mKafkaContainer\u001b[39m = GenericContainer(extraHosts=[], image=RemoteDockerImage(imageName=confluentinc/cp-kafka:5.4.3, imagePullPolicy=DefaultPullPolicy(), imageNameSubstitutor=org.testcontainers.utility.ImageNameSubstitutor$LogWrappedImageNameSubstitutor@2104655a), volumesFroms=[], linkedContainers={}, startupCheckStrategy=org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy@1e012dca, startupAttempts=1, workingDirectory=null, shmSize=null, copyToFileContainerPathMap={}, copyToTransferableContainerPathMap={}, dependencies=[], dockerClient=LazyDockerClient, containerId=1b2790461c4707b170e4ecd66efb7a75461da6e9701ffaec950969fad187f09f, containerInfo=InspectContainerResponse(args=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], config=ContainerConfig(attachStderr=false, attachStdin=false, attachStdout=false, cmd=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], domainName=, entrypoint=[sh], env=[KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807, KAFKA_BROKER_ID=1, KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1, KAFKA_ZOOKEEPER_CONNECT=localhost:2181, KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1, KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1, KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092, KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1, KAFKA_INTER_BROKER_LISTENER_NAME=BROKER, KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT, KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0, PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin, ALLOW_UNSIGNED=false, PYTHON_VERSION=2.7.9-1, SCALA_VERSION=2.12, KAFKA_VERSION=, CONFLUENT_PLATFORM_LABEL=, CONFLUENT_VERSION=5.4.3, CONFLUENT_DEB_VERSION=1, ZULU_OPENJDK_VERSION=8=8.40.0.25, LANG=C.UTF-8, CUB_CLASSPATH=/etc/confluent/docker/docker-utils.jar, KAFKA_ADVERTISED_LISTENERS=, COMPONENT=kafka], exposedPorts=[2181/tcp, 9092/tcp, 9093/tcp], hostName=1b2790461c47, image=confluentinc/cp-kafka:5.4.3, labels={desktop.docker.io/wsl-distro=Ubuntu-24.04, io.confluent.docker=true, io.confluent.docker.build.number=2, io.confluent.docker.git.id=af0a986, io.confluent.docker.git.repo=confluentinc/kafka-images, maintainer=partner-support@confluent.io, org.testcontainers=true, org.testcontainers.lang=java, org.testcontainers.sessionId=bf70e1ba-cb88-4914-9fb1-ec6aa83d589b, org.testcontainers.version=1.19.3}, macAddress=null, networkDisabled=null, onBuild=null, stdinOpen=false, portSpecs=null, stdInOnce=false, tty=false, user=0, volumes={/etc/kafka/secrets={}, /var/lib/kafka/data={}}, workingDir=, healthCheck=null), created=2025-03-30T14:34:29.051884125Z, driver=overlay2, execDriver=null, hostConfig=HostConfig(binds=[], blkioWeight=0, blkioWeightDevice=null, blkioDeviceReadBps=null, blkioDeviceWriteBps=null, blkioDeviceReadIOps=null, blkioDeviceWriteIOps=null, memorySwappiness=null, nanoCPUs=0, capAdd=null, capDrop=null, containerIDFile=, cpuPeriod=0, cpuRealtimePeriod=0, cpuRealtimeRuntime=0, cpuShares=0, cpuQuota=0, cpusetCpus=, cpusetMems=, devices=null, deviceCgroupRules=null, deviceRequests=null, diskQuota=null, dns=null, dnsOptions=nu...\n",
       "\u001b[36mbootstrapServers\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"PLAINTEXT://localhost:58094\"\u001b[39m\n",
       "\u001b[36mkafkaProps\u001b[39m: \u001b[32mProperties\u001b[39m = {value.serializer=org.apache.kafka.common.serialization.StringSerializer, bootstrap.servers=PLAINTEXT://localhost:58094, key.serializer=org.apache.kafka.common.serialization.StringSerializer}\n",
       "\u001b[36mres2_11\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_12\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_13\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mkafkaProducer\u001b[39m: \u001b[32mKafkaProducer\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.kafka.clients.producer.KafkaProducer@219a132b\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msendToKafka\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mreadFromKafkaSpark\u001b[39m\n",
       "\u001b[36mkafkaTopic\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"my-topic-scala\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.kafka.clients.producer._\n",
    "\n",
    "import org.testcontainers.containers.KafkaContainer\n",
    "import org.testcontainers.utility.DockerImageName\n",
    "import java.util.Properties\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"ScalaSpark\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val kafkaContainer = new KafkaContainer()\n",
    "\n",
    "kafkaContainer.start()\n",
    "\n",
    "val bootstrapServers = kafkaContainer.getBootstrapServers\n",
    "val kafkaProps = new Properties()\n",
    "kafkaProps.put(\"bootstrap.servers\", bootstrapServers)\n",
    "kafkaProps.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "kafkaProps.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val kafkaProducer = new KafkaProducer[String, String](kafkaProps)\n",
    "\n",
    "def sendToKafka(topic: String, values: Seq[String]): Unit = {\n",
    " \n",
    "  values.foreach { value =>\n",
    "    val record = new ProducerRecord[String, String](topic, value)\n",
    "    kafkaProducer.send(record)\n",
    "  }\n",
    "  kafkaProducer.flush()\n",
    "}\n",
    "\n",
    "def readFromKafkaSpark(topic: String): DataFrame = {\n",
    "  spark.read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", bootstrapServers)\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load()\n",
    "  .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "}\n",
    "\n",
    "val kafkaTopic = \"my-topic-scala\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:34:35 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {my-topic-scala=LEADER_NOT_AVAILABLE}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"hi\"\u001b[39m, \u001b[32m\"there\"\u001b[39m, \u001b[32m\"from\"\u001b[39m, \u001b[32m\"scala\"\u001b[39m)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"hi\", \"there\", \"from\", \"scala\")\n",
    "\n",
    "sendToKafka(kafkaTopic, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:34:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/30 17:34:38 INFO SharedState: Warehouse path is 'file:/home/eyal/code/notebooks/spark-warehouse'.\n",
      "25/03/30 17:34:39 INFO CodeGenerator: Code generated in 286.722522 ms\n",
      "25/03/30 17:34:39 INFO CodeGenerator: Code generated in 39.929272 ms\n",
      "25/03/30 17:34:41 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:58094]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/03/30 17:34:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/03/30 17:34:41 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 17:34:41 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 17:34:41 INFO AppInfoParser: Kafka startTimeMs: 1743345281657\n",
      "25/03/30 17:34:41 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered\n",
      "25/03/30 17:34:41 INFO Metrics: Metrics scheduler closed\n",
      "25/03/30 17:34:41 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/03/30 17:34:41 INFO Metrics: Metrics reporters closed\n",
      "25/03/30 17:34:41 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(my-topic-scala-0,-2,-1,None)\n",
      "25/03/30 17:34:42 INFO CodeGenerator: Code generated in 24.958951 ms\n",
      "25/03/30 17:34:42 INFO CodeGenerator: Code generated in 17.153792 ms\n",
      "25/03/30 17:34:42 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "25/03/30 17:34:42 INFO DAGScheduler: Got job 0 (show at cmd4.sc:1) with 1 output partitions\n",
      "25/03/30 17:34:42 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd4.sc:1)\n",
      "25/03/30 17:34:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/30 17:34:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/30 17:34:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1), which has no missing parents\n",
      "25/03/30 17:34:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 31.5 KiB, free 2.2 GiB)\n",
      "25/03/30 17:34:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 2.2 GiB)\n",
      "25/03/30 17:34:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:38805 (size: 11.1 KiB, free: 2.2 GiB)\n",
      "25/03/30 17:34:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "25/03/30 17:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/30 17:34:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/03/30 17:34:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 7883 bytes) \n",
      "25/03/30 17:34:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/03/30 17:34:43 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:58094]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/03/30 17:34:43 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 17:34:43 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 17:34:43 INFO AppInfoParser: Kafka startTimeMs: 1743345283420\n",
      "25/03/30 17:34:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Assigned to partition(s): my-topic-scala-0\n",
      "25/03/30 17:34:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 17:34:43 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Cluster ID: 7W4P76jMThyASBlgh4LwFA\n",
      "25/03/30 17:34:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58094 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 17:34:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 17:34:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58094 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 17:34:43 INFO CodeGenerator: Code generated in 20.233912 ms\n",
      "25/03/30 17:34:44 INFO CodeGenerator: Code generated in 20.335683 ms\n",
      "25/03/30 17:34:44 INFO CodeGenerator: Code generated in 7.458552 ms\n",
      "25/03/30 17:34:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Seeking to offset 0 for partition my-topic-scala-0\n",
      "25/03/30 17:34:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 17:34:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58094 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 17:34:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 17:34:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor-1, groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58094 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 17:34:45 INFO CodeGenerator: Code generated in 47.797618 ms\n",
      "25/03/30 17:34:45 INFO KafkaDataConsumer: From Kafka topicPartition=my-topic-scala-0 groupId=spark-kafka-relation-3a498fad-0e14-4ec2-a1bc-8f17aff1bd3d-executor read 4 records through 1 polls (polled  out 4 records), taking 649034513 nanos, during time span of 1632799969 nanos.\n",
      "25/03/30 17:34:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1761 bytes result sent to driver\n",
      "25/03/30 17:34:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2020 ms on 10.255.255.254 (executor driver) (1/1)\n",
      "25/03/30 17:34:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/03/30 17:34:45 INFO DAGScheduler: ResultStage 0 (show at cmd4.sc:1) finished in 2.354 s\n",
      "25/03/30 17:34:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/30 17:34:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/03/30 17:34:45 INFO DAGScheduler: Job 0 finished: show at cmd4.sc:1, took 2.448756 s\n",
      "25/03/30 17:34:45 INFO CodeGenerator: Code generated in 8.626807 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|message|\n",
      "+-------+\n",
      "|     hi|\n",
      "|  there|\n",
      "|   from|\n",
      "|  scala|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readFromKafkaSpark(kafkaTopic).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
