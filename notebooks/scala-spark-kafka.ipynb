{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.0`\n",
    "import $ivy.`org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0`\n",
    "\n",
    "import $ivy.`org.apache.kafka:kafka-clients:3.5.1`\n",
    "import $ivy.`org.testcontainers:testcontainers:1.19.3`\n",
    "import $ivy.`org.testcontainers:kafka:1.19.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/30 16:43:21 WARN Utils: Your hostname, DESKTOP-CN01VS3 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/30 16:43:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/30 16:43:21 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/03/30 16:43:21 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 16:43:21 INFO SparkContext: Java version 17.0.14\n",
      "25/03/30 16:43:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 16:43:21 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 16:43:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/30 16:43:21 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 16:43:21 INFO SparkContext: Submitted application: ScalaSpark\n",
      "25/03/30 16:43:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/30 16:43:21 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/30 16:43:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/30 16:43:21 INFO SecurityManager: Changing view acls to: eyal\n",
      "25/03/30 16:43:21 INFO SecurityManager: Changing modify acls to: eyal\n",
      "25/03/30 16:43:21 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/30 16:43:21 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/30 16:43:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: eyal; groups with view permissions: EMPTY; users with modify permissions: eyal; groups with modify permissions: EMPTY\n",
      "25/03/30 16:43:21 INFO Utils: Successfully started service 'sparkDriver' on port 33339.\n",
      "25/03/30 16:43:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/30 16:43:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/30 16:43:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/30 16:43:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/30 16:43:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/30 16:43:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2dc43953-fa61-43c4-89dc-fd2d11ec6bcd\n",
      "25/03/30 16:43:21 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "25/03/30 16:43:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/30 16:43:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/03/30 16:43:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/30 16:43:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/03/30 16:43:22 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/03/30 16:43:22 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 16:43:22 INFO Executor: Java version 17.0.14\n",
      "25/03/30 16:43:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/30 16:43:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7b6b1de7 for default.\n",
      "25/03/30 16:43:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39171.\n",
      "25/03/30 16:43:22 INFO NettyBlockTransferService: Server created on 10.255.255.254:39171\n",
      "25/03/30 16:43:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/30 16:43:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 39171, None)\n",
      "25/03/30 16:43:22 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:39171 with 2.2 GiB RAM, BlockManagerId(driver, 10.255.255.254, 39171, None)\n",
      "25/03/30 16:43:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 39171, None)\n",
      "25/03/30 16:43:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 39171, None)\n",
      "25/03/30 16:43:22 INFO PullPolicy: Image pull policy will be performed by: DefaultPullPolicy()\n",
      "25/03/30 16:43:22 INFO ImageNameSubstitutor: Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')\n",
      "25/03/30 16:43:22 INFO DockerClientProviderStrategy: Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first\n",
      "25/03/30 16:43:22 INFO DockerClientProviderStrategy: Found Docker environment with local Unix socket (unix:///var/run/docker.sock)\n",
      "25/03/30 16:43:22 INFO DockerClientFactory: Docker host IP address is localhost\n",
      "25/03/30 16:43:22 INFO DockerClientFactory: Connected to docker: \n",
      "  Server Version: 28.0.1\n",
      "  API Version: 1.48\n",
      "  Operating System: Docker Desktop\n",
      "  Total Memory: 15954 MB\n",
      "25/03/30 16:43:22 INFO 1: Creating container for image: testcontainers/ryuk:0.5.1\n",
      "25/03/30 16:43:23 INFO 1: Container testcontainers/ryuk:0.5.1 is starting: ffe98d0d7b65a61ad4c0dc4fb041660e2a10a92e15fb562e3ddc885e08f6dbf9\n",
      "25/03/30 16:43:23 INFO 1: Container testcontainers/ryuk:0.5.1 started in PT0.478368654S\n",
      "25/03/30 16:43:23 INFO RyukResourceReaper: Ryuk started - will monitor and terminate Testcontainers containers on JVM exit\n",
      "25/03/30 16:43:23 INFO DockerClientFactory: Checking the system...\n",
      "25/03/30 16:43:23 INFO DockerClientFactory: ✔︎ Docker server version should be at least 1.6.0\n",
      "25/03/30 16:43:23 INFO 5: Creating container for image: confluentinc/cp-kafka:7.6.5\n",
      "25/03/30 16:43:23 INFO 5: Container confluentinc/cp-kafka:7.6.5 is starting: 24f996cd470cae96c11d3f4a339896af0a352d570c2693216a9f838337d92223\n",
      "25/03/30 16:43:25 INFO 5: Container confluentinc/cp-kafka:7.6.5 started in PT2.032825241S\n",
      "25/03/30 16:43:25 INFO ProducerConfig: ProducerConfig values: \n",
      "\tacks = -1\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbatch.size = 16384\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:57759]\n",
      "\tbuffer.memory = 33554432\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = producer-1\n",
      "\tcompression.type = none\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdelivery.timeout.ms = 120000\n",
      "\tenable.idempotence = true\n",
      "\tinterceptor.classes = []\n",
      "\tkey.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\tlinger.ms = 0\n",
      "\tmax.block.ms = 60000\n",
      "\tmax.in.flight.requests.per.connection = 5\n",
      "\tmax.request.size = 1048576\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetadata.max.idle.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartitioner.adaptive.partitioning.enable = true\n",
      "\tpartitioner.availability.timeout.ms = 0\n",
      "\tpartitioner.class = null\n",
      "\tpartitioner.ignore.keys = false\n",
      "\treceive.buffer.bytes = 32768\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.timeout.ms = 60000\n",
      "\ttransactional.id = null\n",
      "\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n",
      "\n",
      "25/03/30 16:43:25 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.\n",
      "25/03/30 16:43:25 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 16:43:25 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 16:43:25 INFO AppInfoParser: Kafka startTimeMs: 1743342205557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.kafka.clients.producer._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.containers.KafkaContainer\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.testcontainers.utility.DockerImageName\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1458a8eb\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mkafkaContainer\u001b[39m: \u001b[32mKafkaContainer\u001b[39m = GenericContainer(extraHosts=[], image=RemoteDockerImage(imageName=confluentinc/cp-kafka:7.6.5, imagePullPolicy=DefaultPullPolicy(), imageNameSubstitutor=org.testcontainers.utility.ImageNameSubstitutor$LogWrappedImageNameSubstitutor@444205fa), volumesFroms=[], linkedContainers={}, startupCheckStrategy=org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy@25a9c8f7, startupAttempts=1, workingDirectory=null, shmSize=null, copyToFileContainerPathMap={}, copyToTransferableContainerPathMap={}, dependencies=[], dockerClient=LazyDockerClient, containerId=24f996cd470cae96c11d3f4a339896af0a352d570c2693216a9f838337d92223, containerInfo=InspectContainerResponse(args=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], config=ContainerConfig(attachStderr=false, attachStdin=false, attachStdout=false, cmd=[-c, while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh], domainName=, entrypoint=[sh], env=[KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807, KAFKA_BROKER_ID=1, KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1, KAFKA_ZOOKEEPER_CONNECT=localhost:2181, KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1, KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1, KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092, KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1, KAFKA_INTER_BROKER_LISTENER_NAME=BROKER, KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT, KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0, PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin, container=oci, LANG=C.UTF-8, CUB_CLASSPATH=\"/usr/share/java/cp-base-new/*\", KAFKA_ADVERTISED_LISTENERS=, CLUSTER_ID=, COMPONENT=kafka], exposedPorts=[2181/tcp, 9092/tcp, 9093/tcp], hostName=24f996cd470c, image=confluentinc/cp-kafka:7.6.5, labels={architecture=x86_64, build-date=2025-01-21T14:25:54, com.redhat.component=ubi8-minimal-container, com.redhat.license_terms=https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI, description=Common base image for Confluent's Docker images., desktop.docker.io/wsl-distro=Ubuntu-24.04, distribution-scope=public, io.buildah.version=1.33.8, io.confluent.docker=true, io.confluent.docker.build.number=361ed280, io.confluent.docker.git.id=0c8443c, io.confluent.docker.git.repo=confluentinc/kafka-images, io.k8s.description=The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly., io.k8s.display-name=Red Hat Universal Base Image 8 Minimal, io.openshift.expose-services=, io.openshift.tags=minimal rhel8, maintainer=partner-support@confluent.io, name=cp-kafka, org.testcontainers=true, org.testcontainers.lang=java, org.testcontainers.sessionId=065f2e0b-376e-4042-a02a-1d0112a0ccc9, org.testcontainers.version=1.19.3, release=7.6.5-56, summary=Confluent platform Kafka., url=https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8-minimal/images/8.10-...\n",
       "\u001b[36mbootstrapServers\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"PLAINTEXT://localhost:57759\"\u001b[39m\n",
       "\u001b[36mkafkaProps\u001b[39m: \u001b[32mProperties\u001b[39m = {value.serializer=org.apache.kafka.common.serialization.StringSerializer, bootstrap.servers=PLAINTEXT://localhost:57759, key.serializer=org.apache.kafka.common.serialization.StringSerializer}\n",
       "\u001b[36mres2_11\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_12\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres2_13\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mkafkaProducer\u001b[39m: \u001b[32mKafkaProducer\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.kafka.clients.producer.KafkaProducer@3cbc9cec\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msendToKafka\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mreadFromKafkaSpark\u001b[39m\n",
       "\u001b[36mkafkaTopic\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"my-topic-scala\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.kafka.clients.producer._\n",
    "\n",
    "import org.testcontainers.containers.KafkaContainer\n",
    "import org.testcontainers.utility.DockerImageName\n",
    "import java.util.Properties\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"ScalaSpark\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val kafkaContainer = new KafkaContainer(\n",
    "  DockerImageName.parse(\"confluentinc/cp-kafka:7.6.5\")\n",
    ")\n",
    "\n",
    "kafkaContainer.start()\n",
    "\n",
    "val bootstrapServers = kafkaContainer.getBootstrapServers\n",
    "val kafkaProps = new Properties()\n",
    "kafkaProps.put(\"bootstrap.servers\", bootstrapServers)\n",
    "kafkaProps.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "kafkaProps.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val kafkaProducer = new KafkaProducer[String, String](kafkaProps)\n",
    "\n",
    "def sendToKafka(topic: String, values: Seq[String]): Unit = {\n",
    " \n",
    "  values.foreach { value =>\n",
    "    val record = new ProducerRecord[String, String](topic, value)\n",
    "    kafkaProducer.send(record)\n",
    "  }\n",
    "  kafkaProducer.flush()\n",
    "}\n",
    "\n",
    "def readFromKafkaSpark(topic: String): DataFrame = {\n",
    "  spark.read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", bootstrapServers)\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load()\n",
    "  .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "}\n",
    "\n",
    "val kafkaTopic = \"my-topic-scala\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 16:43:25 INFO Metadata: [Producer clientId=producer-1] Cluster ID: V9utvC3-RQqWJ-T04Ee3ng\n",
      "25/03/30 16:43:26 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 0 with epoch 0\n",
      "25/03/30 16:43:26 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 5 : {my-topic-scala=LEADER_NOT_AVAILABLE}\n",
      "25/03/30 16:43:26 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 6 : {my-topic-scala=LEADER_NOT_AVAILABLE}\n",
      "25/03/30 16:43:26 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition my-topic-scala-0 to 0 since the associated topicId changed from null to QAKOnV4MTn-Mm1Gb5o5JjQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"hi\"\u001b[39m, \u001b[32m\"there\"\u001b[39m, \u001b[32m\"from\"\u001b[39m, \u001b[32m\"scala\"\u001b[39m)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"hi\", \"there\", \"from\", \"scala\")\n",
    "\n",
    "sendToKafka(kafkaTopic, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 16:43:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/30 16:43:26 INFO SharedState: Warehouse path is 'file:/home/eyal/code/notebooks/spark-warehouse'.\n",
      "25/03/30 16:43:27 INFO CodeGenerator: Code generated in 175.413951 ms\n",
      "25/03/30 16:43:27 INFO CodeGenerator: Code generated in 22.051742 ms\n",
      "25/03/30 16:43:28 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:57759]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/03/30 16:43:28 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/03/30 16:43:28 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 16:43:28 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 16:43:28 INFO AppInfoParser: Kafka startTimeMs: 1743342208702\n",
      "25/03/30 16:43:28 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered\n",
      "25/03/30 16:43:28 INFO Metrics: Metrics scheduler closed\n",
      "25/03/30 16:43:28 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/03/30 16:43:28 INFO Metrics: Metrics reporters closed\n",
      "25/03/30 16:43:28 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(my-topic-scala-0,-2,-1,None)\n",
      "25/03/30 16:43:29 INFO CodeGenerator: Code generated in 16.258642 ms\n",
      "25/03/30 16:43:29 INFO CodeGenerator: Code generated in 9.491001 ms\n",
      "25/03/30 16:43:29 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Got job 0 (show at cmd4.sc:1) with 1 output partitions\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd4.sc:1)\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1), which has no missing parents\n",
      "25/03/30 16:43:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 31.5 KiB, free 2.2 GiB)\n",
      "25/03/30 16:43:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 2.2 GiB)\n",
      "25/03/30 16:43:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:39171 (size: 11.1 KiB, free: 2.2 GiB)\n",
      "25/03/30 16:43:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "25/03/30 16:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/30 16:43:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/03/30 16:43:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 7883 bytes) \n",
      "25/03/30 16:43:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/03/30 16:43:29 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [PLAINTEXT://localhost:57759]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/03/30 16:43:29 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/03/30 16:43:29 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/03/30 16:43:29 INFO AppInfoParser: Kafka startTimeMs: 1743342209631\n",
      "25/03/30 16:43:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Assigned to partition(s): my-topic-scala-0\n",
      "25/03/30 16:43:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 16:43:29 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Resetting the last seen epoch of partition my-topic-scala-0 to 0 since the associated topicId changed from null to QAKOnV4MTn-Mm1Gb5o5JjQ\n",
      "25/03/30 16:43:29 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Cluster ID: V9utvC3-RQqWJ-T04Ee3ng\n",
      "25/03/30 16:43:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57759 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 16:43:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 16:43:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57759 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 16:43:29 INFO CodeGenerator: Code generated in 11.851378 ms\n",
      "25/03/30 16:43:30 INFO CodeGenerator: Code generated in 17.48024 ms\n",
      "25/03/30 16:43:30 INFO CodeGenerator: Code generated in 6.749846 ms\n",
      "25/03/30 16:43:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Seeking to offset 0 for partition my-topic-scala-0\n",
      "25/03/30 16:43:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Seeking to earliest offset of partition my-topic-scala-0\n",
      "25/03/30 16:43:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57759 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 16:43:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Seeking to latest offset of partition my-topic-scala-0\n",
      "25/03/30 16:43:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor-1, groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor] Resetting offset for partition my-topic-scala-0 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57759 (id: 1 rack: null)], epoch=0}}.\n",
      "25/03/30 16:43:30 INFO CodeGenerator: Code generated in 34.610758 ms\n",
      "25/03/30 16:43:30 INFO KafkaDataConsumer: From Kafka topicPartition=my-topic-scala-0 groupId=spark-kafka-relation-33bb1f45-17a8-42e9-972a-bc647e4893c1-executor read 4 records through 1 polls (polled  out 4 records), taking 605363510 nanos, during time span of 1195166959 nanos.\n",
      "25/03/30 16:43:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1761 bytes result sent to driver\n",
      "25/03/30 16:43:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1487 ms on 10.255.255.254 (executor driver) (1/1)\n",
      "25/03/30 16:43:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/03/30 16:43:30 INFO DAGScheduler: ResultStage 0 (show at cmd4.sc:1) finished in 1.646 s\n",
      "25/03/30 16:43:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/30 16:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/03/30 16:43:30 INFO DAGScheduler: Job 0 finished: show at cmd4.sc:1, took 1.709052 s\n",
      "25/03/30 16:43:30 INFO CodeGenerator: Code generated in 5.874627 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|message|\n",
      "+-------+\n",
      "|     hi|\n",
      "|  there|\n",
      "|   from|\n",
      "|  scala|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readFromKafkaSpark(kafkaTopic).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
