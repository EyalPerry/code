{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.pom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.0`\n",
    "import $ivy.`io.delta:delta-core_2.13:2.1.0`\n",
    "import $ivy.`io.delta:delta-storage:3.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/30 17:42:25 WARN Utils: Your hostname, DESKTOP-CN01VS3 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/30 17:42:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/30 17:42:25 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/03/30 17:42:25 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 17:42:25 INFO SparkContext: Java version 17.0.14\n",
      "25/03/30 17:42:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 17:42:25 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 17:42:25 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/30 17:42:25 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 17:42:25 INFO SparkContext: Submitted application: ScalaSpark\n",
      "25/03/30 17:42:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/30 17:42:25 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/30 17:42:25 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/30 17:42:25 INFO SecurityManager: Changing view acls to: eyal\n",
      "25/03/30 17:42:25 INFO SecurityManager: Changing modify acls to: eyal\n",
      "25/03/30 17:42:25 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/30 17:42:25 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/30 17:42:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: eyal; groups with view permissions: EMPTY; users with modify permissions: eyal; groups with modify permissions: EMPTY\n",
      "25/03/30 17:42:25 INFO Utils: Successfully started service 'sparkDriver' on port 40085.\n",
      "25/03/30 17:42:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/30 17:42:25 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/30 17:42:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/30 17:42:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/30 17:42:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/30 17:42:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f30ef186-fe79-4c04-bdd7-468a2dbcd950\n",
      "25/03/30 17:42:25 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "25/03/30 17:42:26 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/30 17:42:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/03/30 17:42:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/30 17:42:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/30 17:42:26 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "25/03/30 17:42:26 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/03/30 17:42:26 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 17:42:26 INFO Executor: Java version 17.0.14\n",
      "25/03/30 17:42:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/30 17:42:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3a91f854 for default.\n",
      "25/03/30 17:42:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46181.\n",
      "25/03/30 17:42:26 INFO NettyBlockTransferService: Server created on 10.255.255.254:46181\n",
      "25/03/30 17:42:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/30 17:42:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 46181, None)\n",
      "25/03/30 17:42:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:46181 with 2.2 GiB RAM, BlockManagerId(driver, 10.255.255.254, 46181, None)\n",
      "25/03/30 17:42:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 46181, None)\n",
      "25/03/30 17:42:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 46181, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@12884c74\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mtablePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../local/delta-scala\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"ScalaSpark\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val tablePath = \"../local/delta-scala\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:42:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/30 17:42:31 INFO SharedState: Warehouse path is 'file:/home/eyal/code/notebooks/spark-warehouse'.\n",
      "25/03/30 17:42:31 INFO CodeGenerator: Code generated in 174.277318 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [name: string, age: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (\"Alice\", 25),\n",
    "  (\"Bob\", 30)\n",
    ").toDF(\"name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:42:33 INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`\n",
      "25/03/30 17:42:33 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
      "25/03/30 17:42:33 INFO InitialSnapshot: [tableId=cae407d7-f160-420e-9df8-e1c3148ba5e2] Created snapshot InitialSnapshot(path=file:/home/eyal/code/local/delta-scala/_delta_log, version=-1, metadata=Metadata(af25a023-1919-4b19-b88a-f556836aab0d,null,null,Format(parquet,Map()),null,List(),Map(),Some(1743345753367)), logSegment=LogSegment(file:/home/eyal/code/local/delta-scala/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)\n"
     ]
    },
    {
     "ename": "java.lang.NoClassDefFoundError",
     "evalue": "org/apache/spark/sql/execution/datasources/FileFormatWriter$Empty2Null",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.NoClassDefFoundError: org/apache/spark/sql/execution/datasources/FileFormatWriter$Empty2Null\u001b[39m",
      "  org.apache.spark.sql.delta.DeltaLog.startTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m206\u001b[39m)",
      "  org.apache.spark.sql.delta.DeltaLog.withNewTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.delta.commands.WriteIntoDelta.run(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(\u001b[32mDeltaDataSource.scala\u001b[39m:\u001b[32m159\u001b[39m)",
      "  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(\u001b[32mSaveIntoDataSourceCommand.scala\u001b[39m:\u001b[32m48\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m75\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m73\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m201\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m108\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m66\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m263\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m437\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m85\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m142\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m859\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m388\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveInternal(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m304\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m240\u001b[39m)",
      "  ammonite.$sess.cmd4$Helper.<init>(\u001b[32mcmd4.sc\u001b[39m:\u001b[32m4\u001b[39m)",
      "  ammonite.$sess.cmd4$.<clinit>(\u001b[32mcmd4.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "\u001b[31mjava.lang.ClassNotFoundException: org.apache.spark.sql.execution.datasources.FileFormatWriter$Empty2Null\u001b[39m",
      "  java.net.URLClassLoader.findClass(\u001b[32mURLClassLoader.java\u001b[39m:\u001b[32m445\u001b[39m)",
      "  ammonite.runtime.SpecialClassLoader.findClass(\u001b[32mClassLoaders.scala\u001b[39m:\u001b[32m250\u001b[39m)",
      "  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m592\u001b[39m)",
      "  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m525\u001b[39m)",
      "  org.apache.spark.sql.delta.DeltaLog.startTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m206\u001b[39m)",
      "  org.apache.spark.sql.delta.DeltaLog.withNewTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.delta.commands.WriteIntoDelta.run(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(\u001b[32mDeltaDataSource.scala\u001b[39m:\u001b[32m159\u001b[39m)",
      "  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(\u001b[32mSaveIntoDataSourceCommand.scala\u001b[39m:\u001b[32m48\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m75\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m73\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m201\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m108\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m66\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m461\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m263\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m437\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m85\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m142\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m859\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m388\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveInternal(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m304\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m240\u001b[39m)",
      "  ammonite.$sess.cmd4$Helper.<init>(\u001b[32mcmd4.sc\u001b[39m:\u001b[32m4\u001b[39m)",
      "  ammonite.$sess.cmd4$.<clinit>(\u001b[32mcmd4.sc\u001b[39m:\u001b[32m7\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")  // or append\n",
    "  .save(tablePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
